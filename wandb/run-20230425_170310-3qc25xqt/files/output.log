gpu_info:
[32mcuda.is_available:True
[32mcuda.device_count:1
[32mcuda.device_name:NVIDIA GeForce RTX 3050 Laptop GPU
[32mcuda.current_device:0
datasets_info:
[32mtrain_inputs:2816	train_labels:2816
[32mval_inputs:705	val_labels:705
[32mlabel_map:{'Áï™ËåÑÂè∂ÊñëÁóÖ': 0, 'ËãπÊûúÈªëÊòüÁóÖ': 1, 'Ëë°ËêÑÈªëËÖêÁóÖ': 2}
Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã
C:\Users\yujunyu\.conda\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
C:\Users\yujunyu\.conda\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
net_structure:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,864
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
             ReLU-10           [-1, 64, 56, 56]               0
       BasicBlock-11           [-1, 64, 56, 56]               0
           Conv2d-12           [-1, 64, 56, 56]          36,864
      BatchNorm2d-13           [-1, 64, 56, 56]             128
             ReLU-14           [-1, 64, 56, 56]               0
           Conv2d-15           [-1, 64, 56, 56]          36,864
      BatchNorm2d-16           [-1, 64, 56, 56]             128
             ReLU-17           [-1, 64, 56, 56]               0
       BasicBlock-18           [-1, 64, 56, 56]               0
           Conv2d-19          [-1, 128, 28, 28]          73,728
      BatchNorm2d-20          [-1, 128, 28, 28]             256
             ReLU-21          [-1, 128, 28, 28]               0
           Conv2d-22          [-1, 128, 28, 28]         147,456
      BatchNorm2d-23          [-1, 128, 28, 28]             256
           Conv2d-24          [-1, 128, 28, 28]           8,192
      BatchNorm2d-25          [-1, 128, 28, 28]             256
             ReLU-26          [-1, 128, 28, 28]               0
       BasicBlock-27          [-1, 128, 28, 28]               0
           Conv2d-28          [-1, 128, 28, 28]         147,456
      BatchNorm2d-29          [-1, 128, 28, 28]             256
             ReLU-30          [-1, 128, 28, 28]               0
           Conv2d-31          [-1, 128, 28, 28]         147,456
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
       BasicBlock-34          [-1, 128, 28, 28]               0
           Conv2d-35          [-1, 256, 14, 14]         294,912
      BatchNorm2d-36          [-1, 256, 14, 14]             512
             ReLU-37          [-1, 256, 14, 14]               0
           Conv2d-38          [-1, 256, 14, 14]         589,824
      BatchNorm2d-39          [-1, 256, 14, 14]             512
           Conv2d-40          [-1, 256, 14, 14]          32,768
      BatchNorm2d-41          [-1, 256, 14, 14]             512
             ReLU-42          [-1, 256, 14, 14]               0
       BasicBlock-43          [-1, 256, 14, 14]               0
           Conv2d-44          [-1, 256, 14, 14]         589,824
      BatchNorm2d-45          [-1, 256, 14, 14]             512
             ReLU-46          [-1, 256, 14, 14]               0
           Conv2d-47          [-1, 256, 14, 14]         589,824
      BatchNorm2d-48          [-1, 256, 14, 14]             512
             ReLU-49          [-1, 256, 14, 14]               0
       BasicBlock-50          [-1, 256, 14, 14]               0
           Conv2d-51            [-1, 512, 7, 7]       1,179,648
      BatchNorm2d-52            [-1, 512, 7, 7]           1,024
             ReLU-53            [-1, 512, 7, 7]               0
           Conv2d-54            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-55            [-1, 512, 7, 7]           1,024
           Conv2d-56            [-1, 512, 7, 7]         131,072
      BatchNorm2d-57            [-1, 512, 7, 7]           1,024
             ReLU-58            [-1, 512, 7, 7]               0
       BasicBlock-59            [-1, 512, 7, 7]               0
           Conv2d-60            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-61            [-1, 512, 7, 7]           1,024
             ReLU-62            [-1, 512, 7, 7]               0
           Conv2d-63            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-64            [-1, 512, 7, 7]           1,024
             ReLU-65            [-1, 512, 7, 7]               0
       BasicBlock-66            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0
           Linear-68                    [-1, 3]           1,539
================================================================
Total params: 11,178,051
Trainable params: 11,178,051
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 62.79
Params size (MB): 42.64
Estimated Total Size (MB): 106.00
----------------------------------------------------------------
[32mResNet(
[32m  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
[32m  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m  (relu): ReLU(inplace=True)
[32m  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
[32m  (layer1): Sequential(
[32m    (0): BasicBlock(
[32m      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m    )
[32m    (1): BasicBlock(
[32m      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m    )
[32m  )
[32m  (layer2): Sequential(
[32m    (0): BasicBlock(
[32m      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (downsample): Sequential(
[32m        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
[32m        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      )
[32m    )
[32m    (1): BasicBlock(
[32m      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m    )
[32m  )
[32m  (layer3): Sequential(
[32m    (0): BasicBlock(
[32m      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (downsample): Sequential(
[32m        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
[32m        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      )
[32m    )
[32m    (1): BasicBlock(
[32m      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m    )
[32m  )
[32m  (layer4): Sequential(
[32m    (0): BasicBlock(
[32m      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (downsample): Sequential(
[32m        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
[32m        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      )
[32m    )
[32m    (1): BasicBlock(
[32m      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m      (relu): ReLU(inplace=True)
[32m      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
[32m      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[32m    )
[32m  )
[32m  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
[32m  (fc): Linear(in_features=512, out_features=3, bias=True)
[32m)
ÂàùÂßãÂåñÁöÑÂ≠¶‰π†ÁéáÔºö 0.001
[34mÂºÄÂßãËÆ≠ÁªÉ......
Á¨¨1‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.001000
epoch:1/50 	 train_acc:95.88068389892578 	 val_acc:38.15602493286133 	 train_loss:0.0018940785666927695 	 val_loss:0.0
Á¨¨2‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000999
epoch:2/50 	 train_acc:98.65056610107422 	 val_acc:98.72339630126953 	 train_loss:0.0054573179222643375 	 val_loss:1.7832326193456538e-05
Á¨¨3‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000996
epoch:3/50 	 train_acc:99.78693389892578 	 val_acc:99.574462890625 	 train_loss:0.0006911669042892754 	 val_loss:1.4176438298818539e-06
Á¨¨4‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000991
epoch:4/50 	 train_acc:99.89347076416016 	 val_acc:99.99999237060547 	 train_loss:0.0002817276690620929 	 val_loss:0.00024948830832727253
Á¨¨5‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000984
epoch:5/50 	 train_acc:99.96449279785156 	 val_acc:99.858154296875 	 train_loss:7.246722816489637e-05 	 val_loss:3.573230787878856e-05
Á¨¨6‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000976
epoch:6/50 	 train_acc:99.96449279785156 	 val_acc:99.99999237060547 	 train_loss:0.000610613904427737 	 val_loss:2.2652073312201537e-05
Á¨¨7‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000965
epoch:7/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:0.00012054630497004837 	 val_loss:1.2556491128634661e-05
Á¨¨8‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000952
epoch:8/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.7137223039753735e-05 	 val_loss:4.658016223402228e-06
Á¨¨9‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000938
epoch:9/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:3.393045699340291e-05 	 val_loss:6.64571552988491e-06
Á¨¨10‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000922
epoch:10/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.7903639673022553e-05 	 val_loss:4.126213298150105e-06
Á¨¨11‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000905
epoch:11/50 	 train_acc:100.0 	 val_acc:99.858154296875 	 train_loss:4.823602648684755e-05 	 val_loss:5.819141733809374e-06
Á¨¨12‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000885
epoch:12/50 	 train_acc:99.64488983154297 	 val_acc:91.9148941040039 	 train_loss:0.0012420975835993886 	 val_loss:0.6511124968528748
Á¨¨13‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000864
epoch:13/50 	 train_acc:99.78693389892578 	 val_acc:99.71630859375 	 train_loss:0.0005221219034865499 	 val_loss:0.00035248868516646326
Á¨¨14‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000842
epoch:14/50 	 train_acc:99.78693389892578 	 val_acc:97.44680786132812 	 train_loss:0.001447522547096014 	 val_loss:1.7170599676319398e-05
Á¨¨15‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000819
epoch:15/50 	 train_acc:99.78693389892578 	 val_acc:99.574462890625 	 train_loss:0.003196840640157461 	 val_loss:2.439199704440398e-07
Á¨¨16‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000794
epoch:16/50 	 train_acc:99.5383529663086 	 val_acc:99.14893341064453 	 train_loss:0.04535306990146637 	 val_loss:0.06014426425099373
Á¨¨17‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000768
epoch:17/50 	 train_acc:99.6803970336914 	 val_acc:98.86524963378906 	 train_loss:0.0011472151381894946 	 val_loss:6.602349600370871e-08
Á¨¨18‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000741
epoch:18/50 	 train_acc:100.0 	 val_acc:99.858154296875 	 train_loss:0.0005687447846867144 	 val_loss:4.401571729317766e-08
Á¨¨19‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000713
epoch:19/50 	 train_acc:99.9289779663086 	 val_acc:99.99999237060547 	 train_loss:7.321962766582146e-05 	 val_loss:5.308863819664111e-06
Á¨¨20‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000684
epoch:20/50 	 train_acc:99.9289779663086 	 val_acc:99.71630859375 	 train_loss:0.0002004977286560461 	 val_loss:9.315810530097224e-06
Á¨¨21‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000655
epoch:21/50 	 train_acc:99.9289779663086 	 val_acc:99.858154296875 	 train_loss:0.0004125011037103832 	 val_loss:2.8976864996366203e-07
Á¨¨22‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000624
epoch:22/50 	 train_acc:99.85795593261719 	 val_acc:99.858154296875 	 train_loss:0.0002946366148535162 	 val_loss:4.584520411299309e-06
Á¨¨23‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000594
epoch:23/50 	 train_acc:99.9289779663086 	 val_acc:99.99999237060547 	 train_loss:0.003803107887506485 	 val_loss:7.260383881657617e-06
Á¨¨24‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000563
epoch:24/50 	 train_acc:99.96449279785156 	 val_acc:99.858154296875 	 train_loss:0.00011829343566205353 	 val_loss:8.89057537278859e-06
Á¨¨25‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000531
epoch:25/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:0.00011615266703302041 	 val_loss:1.6358867469534744e-06
Á¨¨26‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000500
epoch:26/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:7.059836207190529e-05 	 val_loss:5.177026196179213e-06
Á¨¨27‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000469
epoch:27/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:4.755848931381479e-05 	 val_loss:1.0544357792241499e-05
Á¨¨28‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000437
epoch:28/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:4.7680343413958326e-05 	 val_loss:1.0056501196231693e-05
Á¨¨29‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000406
epoch:29/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:8.380685358133633e-06 	 val_loss:7.074934728734661e-06
Á¨¨30‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000376
epoch:30/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:3.673891114885919e-05 	 val_loss:5.47226090930053e-06
Á¨¨31‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000345
epoch:31/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:9.187004252453335e-06 	 val_loss:3.530285766828456e-06
Á¨¨32‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000316
epoch:32/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:0.00012610078556463122 	 val_loss:2.4904668407543795e-06
Á¨¨33‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000287
epoch:33/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:4.425568931765156e-06 	 val_loss:2.1016910523030674e-06
Á¨¨34‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000259
epoch:34/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.1627585081441794e-05 	 val_loss:2.45013075073075e-06
Á¨¨35‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000232
epoch:35/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.682999027252663e-05 	 val_loss:1.305779505855753e-06
Á¨¨36‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000206
epoch:36/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:6.674341875623213e-06 	 val_loss:1.1810658406830044e-06
Á¨¨37‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000181
epoch:37/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:2.324203887837939e-05 	 val_loss:1.0710233482313924e-06
Á¨¨38‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000158
epoch:38/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:9.125718497671187e-05 	 val_loss:1.35710774884501e-06
Á¨¨39‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000136
epoch:39/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.2946830793225672e-05 	 val_loss:1.2360729897409328e-06
Á¨¨40‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000115
epoch:40/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.3548437891586218e-05 	 val_loss:1.1957287142649875e-06
Á¨¨41‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000095
epoch:41/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:8.36848539620405e-06 	 val_loss:1.3369389080253313e-06
Á¨¨42‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000078
epoch:42/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.3000728358747438e-05 	 val_loss:9.261456739295681e-07
Á¨¨43‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000062
epoch:43/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:3.782486237469129e-05 	 val_loss:1.1370447055014665e-06
Á¨¨44‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000048
epoch:44/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:5.578566560870968e-05 	 val_loss:9.793291155801853e-07
Á¨¨45‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000035
epoch:45/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:0.00019015291763935238 	 val_loss:1.4378006198967341e-06
Á¨¨46‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000024
epoch:46/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:4.425017687026411e-05 	 val_loss:1.0856964536287705e-06
Á¨¨47‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000016
epoch:47/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:5.335423338692635e-06 	 val_loss:9.059731951310823e-07
Á¨¨48‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000009
epoch:48/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:1.5051769878482446e-05 	 val_loss:1.193895286633051e-06
Á¨¨49‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000004
epoch:49/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:2.5375089535373263e-05 	 val_loss:1.0581866263237316e-06
Á¨¨50‰∏™epochÁöÑÂ≠¶‰π†ÁéáÔºö0.000001
epoch:50/50 	 train_acc:100.0 	 val_acc:99.99999237060547 	 train_loss:7.751761586405337e-06 	 val_loss:8.619586537861323e-07
Ê®°Âûã‰øùÂ≠òÊàêÂäü:model.pth
ËÆ≠ÁªÉÂÆåÊàê